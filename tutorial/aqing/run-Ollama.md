# Local LLM
他媽的這件事情應該並沒有這麼容易，但總之先動起來再說？

## Steps
1. 瀏覽器搜尋 `Ollama`
2. 安裝對應作業系統的版本，安朱成功應該會出現一隻**羊駝** (哈？）
3. 打開 terminal
4. 輸入這個：
```
ollama run Llama2
```
5. 然後第一次要等他下載一些東西
6. 下載完成就會進入對話介面（終端長什麼樣子對話介面就長什麼樣子）（好像也有 Ollama 自帶的介面但終端比較帥）
7. 跟他對話，發現他有點笨笨的
8. `Ctrl + D` 退出對話，他不會記得上個對話中的東西，次拋語言模型...


## WHY
（好啦我之後寫完整一點）

「模型」包含框架和參數（權重檔）  
載下來的權重檔要有工具可以讀他

Ollama 大概來說就像是 CASA, DS9 這類的，讀特定檔案的東西  
但 Ollama 的的是語言模型的參數檔 啊天文小工具們讀的是 .fits  

Ollama 裡面包含 llama.cpp，但 Ollama 做了類似封裝的動作  
比較沒有那麼露骨？

提供本地的 api  
應該是本地啦，因為斷網網也能用  
但偏笨
