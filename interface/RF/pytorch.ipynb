{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f88bba8",
   "metadata": {},
   "source": [
    "## Practice Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9237fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding:\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def to_onehot(y, num_classes):\n",
    "    y_onehot = torch.zeros(y.size(0), num_classes)\n",
    "    y_onehot.scatter_(1, y.view(-1, 1).long(), 1).float()\n",
    "    return y_onehot\n",
    "\n",
    "y = torch.tensor([0, 1, 2, 2])\n",
    "\n",
    "y_enc = to_onehot(y, 3)\n",
    "\n",
    "print('one-hot encoding:\\n', y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74495fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3000, -0.5000, -0.5000],\n",
       "        [-0.4000, -0.1000, -0.5000],\n",
       "        [-0.3000, -0.9400, -0.5000],\n",
       "        [-0.9900, -0.8800, -0.5000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = torch.tensor( [[-0.3,  -0.5, -0.5],\n",
    "                   [-0.4,  -0.1, -0.5],\n",
    "                   [-0.3,  -0.94, -0.5],\n",
    "                   [-0.99, -0.88, -0.5]])\n",
    "\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf9632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:\n",
      " tensor([[0.3792, 0.3104, 0.3104],\n",
      "        [0.3072, 0.4147, 0.2780],\n",
      "        [0.4263, 0.2248, 0.3490],\n",
      "        [0.2668, 0.2978, 0.4354]])\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return (torch.exp(z.t()) / torch.sum(torch.exp(z), dim=1)).t()\n",
    "\n",
    "smax = softmax(Z)\n",
    "print('softmax:\\n', smax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8010976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels:  tensor([0, 1, 0, 2])\n",
      "true class labels:  tensor([0, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "def to_classlabel(z):\n",
    "    return torch.argmax(z, dim=1)\n",
    "\n",
    "print('predicted class labels: ', to_classlabel(smax))\n",
    "print('true class labels: ', to_classlabel(y_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e342af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: tensor([0.9698, 0.8801, 1.0527, 0.8314])\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(softmax, y_target):\n",
    "    return - torch.sum(torch.log(softmax) * (y_target), dim=1)\n",
    "\n",
    "xent = cross_entropy(smax, y_enc)\n",
    "print('Cross Entropy:', xent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549a340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9698, 0.8801, 1.0527, 0.8314])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(torch.log(smax), y, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f4710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9698, 0.8801, 1.0527, 0.8314])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(Z, y, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5cb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9335)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(Z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5b10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9335)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(cross_entropy(smax, y_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc2424",
   "metadata": {},
   "source": [
    "## G學長教我的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72946e7e",
   "metadata": {},
   "source": [
    "#### 一、張量(Tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71d662",
   "metadata": {},
   "source": [
    "從從容容，就是最熟悉的純量、向量、矩陣  \n",
    "- 0階張量: scalar\n",
    "- 1階張量: vector\n",
    "- 2階張量: Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee09e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x = x.to(\"cuda\")                # Move tensor to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31effed",
   "metadata": {},
   "source": [
    "##### 簡單的張量計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6067c156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# create a 2D tensor\n",
    "tensor_2d = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6]])\n",
    "print(tensor_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d5fe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# check the shape\n",
    "print(tensor_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7662693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# reshape the tensor to 3x2\n",
    "# method 1\n",
    "print(tensor_2d.reshape(3, 2))\n",
    "\n",
    "# **method 2** : use `view`\n",
    "print(tensor_2d.view(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199823a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# transpose the tensor\n",
    "print(tensor_2d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d4998a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n",
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# muptliply two tensors\n",
    "tensor_a = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6]])\n",
    "tensor_b = tensor_a.T\n",
    "\n",
    "# meethod 1\n",
    "print(tensor_a.matmul(tensor_b))\n",
    "\n",
    "# method 2: use `@` operator\n",
    "print(tensor_a @ tensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a7db6",
   "metadata": {},
   "source": [
    "### 二、計算圖(computation Graph)和模組(Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72d761",
   "metadata": {},
   "source": [
    "建立一個神經網路，就是一連串的數學運算流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e90e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLLM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(10, 5)    # 定義零件\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer1(x)                   # 定義資料流向"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34b81c",
   "metadata": {},
   "source": [
    "##### 實作一個簡單的邏輯斯回歸分類器(視為一個簡單的單層神經網路)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d220b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])         # answer\n",
    "x1 = torch.tensor([1.1])        # 輸入特徵\n",
    "w1 = torch.tensor([2.2])        # weights\n",
    "b = torch.tensor([0.0])         # bias unit\n",
    "\n",
    "z = x1 * w1 + b         # input\n",
    "a = torch.sigmoid(z)    # activation function and output\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y) # 計算損失\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40796a5c",
   "metadata": {},
   "source": [
    "Autograd(自動微分): 可幫我們回朔整個過程，一次次的校正參數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f5f40",
   "metadata": {},
   "source": [
    "##### 加入自動微分引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "336fcd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b \n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True) \n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7c9fcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "# 呼叫backward計算梯度\n",
    "\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b8876",
   "metadata": {},
   "source": [
    "一些簡單對於神經網路的小小解釋(fig.A.9)\n",
    "1. 每個單元需拉線到下層的每一個單元，是因為要確保該單元與下層要提取的每個特徵之間的關聯性多強；所以連線也是代表權重的意思(w)\n",
    "2. 偏值單元(bias unit)就是y=ax+b的b，可以更靈活的去擬合資料\n",
    "3. 最終輸出有幾個單元，取決於一開始給的分類任務\n",
    "4. 一層一層像是一次又一次的濃縮精華，由最後濃縮出來的東西做決定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43212eff",
   "metadata": {},
   "source": [
    "## 五大步驟: \n",
    "1. 算結果: 將資料丟進模型，算出預測值\n",
    "2. 算分數: 比較預測值和標準答案差多少\n",
    "3. 清空紀錄: 把上一次的修正建議忘記，避免干擾這一次的\n",
    "4. 找方向: 叫助手回溯過程，算出這一次要怎麼修正\n",
    "5. 更新: 依照修正建議，實際去調整模型的參數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb4011",
   "metadata": {},
   "source": [
    "### 實作多層神經網路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fae806",
   "metadata": {},
   "source": [
    "##### 具有兩層隱藏層的多層感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "345dbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs): \n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),                    # add activation function\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),            # 頭尾數字一樣\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ca2e3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(50, 3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a1415be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable model parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a641d4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1094, -0.0264,  0.0301,  ...,  0.0840, -0.0410, -0.1363],\n",
      "        [-0.1253, -0.1362, -0.1288,  ...,  0.0990, -0.0125,  0.0207],\n",
      "        [-0.0715, -0.0232, -0.0622,  ...,  0.0447,  0.0237, -0.1273],\n",
      "        ...,\n",
      "        [ 0.0008,  0.0187,  0.0432,  ..., -0.0546,  0.1068, -0.0161],\n",
      "        [ 0.0481, -0.0893,  0.0443,  ...,  0.0828,  0.0979,  0.0907],\n",
      "        [ 0.1028, -0.0143,  0.0915,  ..., -0.0774, -0.1133, -0.0102]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a51a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d4883e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)          # 選擇隨機初始權重\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a1188e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)      # 選擇隨機初始權重\n",
    "X = torch.rand((1, 50))     # input data\n",
    "out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f75bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "# 不進行訓練或反向傳播，可節省記憶體\n",
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)              # output為原始數值，未經標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdbdbd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)    # 用softmax標準化，輸出即為機率\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff547b8",
   "metadata": {},
   "source": [
    "### A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a43529e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練集\n",
    "X_train = torch.tensor([                # 5個訓練樣本，每個樣本包含2個特徵\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "    ])\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1]) # 5個訓練樣本對應的標籤\n",
    "\n",
    "# 測試集\n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d92ea7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):           # 初始設定，將準備好的資料(X)(特徵)和答案(y)輸入\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):       # 取得某一筆資料，告訴index幾，取出幾號資料\n",
    "        one_x = self.features[index] \n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "    \n",
    "    def __len__(self):                  # 資料總長度\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bebe204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e1e4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(123)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,           # 每次端幾筆資料給model吃\n",
    "    shuffle=True,           # 洗牌，每次取的資料順序不同，防止模型背答案\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,          # 對於測試集只是要打分數沒必要洗牌\n",
    "     num_workers=0 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431aa4b",
   "metadata": {},
   "source": [
    "討論`num_workers=0`的情況  \n",
    "此參數被設定為0時，資料載入會在主處理程序中進行，而不是在單獨的工作處理程序中進行  \n",
    "若設為1或更大的數字，可以平行處理事情，更有效的利用系統資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dabca1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]])\n",
      "tensor([1, 0])\n",
      "Batch 2:\n",
      "tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]])\n",
      "tensor([0, 0])\n",
      "Batch 3:\n",
      "tensor([[ 2.7000, -1.5000]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# 印出所有batches的結果\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73cae71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]])\n",
      "tensor([1, 0])\n",
      "Batch 2:\n",
      "tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# 注意到最後一個batch只有1筆資料，如果訓練週期中最後一個批次遠小於其他批次可能會影響訓練的收斂性\n",
    "# 設定drop_last=True可以捨棄最後一個batch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,          \n",
    "    shuffle=True,          \n",
    "    drop_last=True          #捨棄最後一個Batch\n",
    ")\n",
    "\n",
    "# 發現最後一個被丟掉了\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7bf27",
   "metadata": {},
   "source": [
    "### A.7 典型訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "640d63b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 3      # 代表要訓練多少個週期(要把所有訓練資料跑三次)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()   # 訓練模式on\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        logits = model(features)                                    # step 1: 猜答案(Forward pass)\n",
    "        loss = F.cross_entropy(logits, labels)                      # step 2: 算分數(Calculate loss)\n",
    "\n",
    "        optimizer.zero_grad()                                       # step 3: 將前一輪的梯度設為0(Zero gradients)\n",
    "        loss.backward()                                             # step 4: 尋找修正方向(Backward pass)(自動微分)\n",
    "        optimizer.step()                                            # step 5: 實際修正(Optimizer step)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "        \n",
    "    model.eval()    # 評估模式on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53b12aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():           # 關閉記錄功能(因為現在只要預測)\n",
    "    outputs = model(X_train)    # 寫考古題\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54bab43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n",
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)      # 關掉科學記號\n",
    "probas = torch.softmax(outputs, dim=1)      # 分數轉機率\n",
    "print(probas)\n",
    "\n",
    "predictions = torch.argmax(probas, dim=1)   # 回傳機率最大的類別\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5c3fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# 可以直接把argmax直接運用在logits上，就不用softmax這步了\n",
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c9bfbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確認是否和答案一樣\n",
    "predictions == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "021d12b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#利用torch.sum計算正確的數量\n",
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "445dfcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把上述預測準確率的過程更通用，所以把他打包成compute_accuracy函式\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model = model.eval()\n",
    "    correct = 0.0       # 目前答對幾題\n",
    "    total_examples = 0  # 目前總共考幾題\n",
    "    \n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        \n",
    "        with torch.no_grad():   # 預測答案\n",
    "            logits = model(features)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        compare = labels == predictions         # 對答案進行比對、累加\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item()    # 回傳正確率(答對題數/總題數)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cd7c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(model, train_loader))\n",
    "print(compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f8cb9",
   "metadata": {},
   "source": [
    "### A.8 儲存與載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "379cc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先進行儲存的動作，存到硬碟裡面\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "862e40e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_36164\\3094577948.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))  # 讀檔，就不用重新訓練了，可以直接預測\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 從硬碟讀取已儲存的模型參數\n",
    "model = NeuralNetwork(2, 2)                     # 建立一個一樣架構的空模型給模型住進去，(2,2)需要和當初訓練的模型一致\n",
    "model.load_state_dict(torch.load(\"model.pth\"))  # 讀檔，就不用重新訓練了，可以直接預測"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539330e",
   "metadata": {},
   "source": [
    "### A.9 使用GPU優化訓練效能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0bb4c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 確認環境可不可以使用GPU\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c02de96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Re: 用CPU執行兩個張量相加\n",
    "tensor_1 = torch.tensor([1., 2., 3.])\n",
    "tensor_2 = torch.tensor([4., 5., 6.])\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a971a156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 利用.to()將張量移到GPU上面\n",
    "tensor_1 = tensor_1.to(\"cuda:0\")\n",
    "tensor_2 = tensor_2.to(\"cuda:0\")\n",
    "print(tensor_1 + tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8bff972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "# 將訓練迴圈用GPU跑\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "#---------------------------------------------------------#\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "#---------------------------------------------------------#\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "#---------------------------------------------------------#\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "#---------------------------------------------------------#        \n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "        \n",
    "    model.eval()    # 評估模式on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d12a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
